"""
Evaluation Metrics Module

This module provides metrics for evaluating contract analysis performance.
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum


class MetricType(Enum):
    """Types of evaluation metrics"""
    ACCURACY = "accuracy"
    PRECISION = "precision"
    RECALL = "recall"
    F1_SCORE = "f1_score"
    HALLUCINATION_RATE = "hallucination_rate"
    CITATION_ACCURACY = "citation_accuracy"
    RESPONSE_TIME = "response_time"
    TOKEN_USAGE = "token_usage"
    COST = "cost"


@dataclass
class MetricValue:
    """A single metric value"""
    name: str
    value: float
    type: MetricType
    unit: Optional[str] = None


@dataclass
class EvaluationResult:
    """Complete evaluation results"""
    metrics: Dict[MetricType, MetricValue]
    model_name: str
    contract_count: int
    timestamp: str


class EvaluationMetrics:
    """Compute evaluation metrics for contract analysis"""

    @staticmethod
    def accuracy(predictions: List[str], ground_truth: List[str]) -> float:
        """
        Calculate accuracy: correct predictions / total predictions

        Args:
            predictions: List of predicted values
            ground_truth: List of true values

        Returns:
            Accuracy score (0-1)
        """
        if len(predictions) != len(ground_truth):
            raise ValueError("Predictions and ground truth must have same length")

        correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)
        return correct / len(predictions) if predictions else 0.0

    @staticmethod
    def precision(
        predicted_risks: List[Dict[str, Any]],
        actual_risks: List[Dict[str, Any]]
    ) -> float:
        """
        Calculate precision: TP / (TP + FP)

        For risk detection:
        - TP: Correctly identified risk
        - FP: Incorrectly identified risk

        Args:
            predicted_risks: List of predicted risks
            actual_risks: List of actual risks

        Returns:
            Precision score (0-1)
        """
        if not predicted_risks:
            return 0.0

        # Match predicted risks with actual risks by category
        actual_categories = {risk['category'] for risk in actual_risks}
        tp = sum(1 for risk in predicted_risks if risk.get('category') in actual_categories)
        fp = len(predicted_risks) - tp

        return tp / (tp + fp) if (tp + fp) > 0 else 0.0

    @staticmethod
    def recall(
        predicted_risks: List[Dict[str, Any]],
        actual_risks: List[Dict[str, Any]]
    ) -> float:
        """
        Calculate recall: TP / (TP + FN)

        For risk detection:
        - TP: Correctly identified risk
        - FN: Missed risk

        Args:
            predicted_risks: List of predicted risks
            actual_risks: List of actual risks

        Returns:
            Recall score (0-1)
        """
        if not actual_risks:
            return 0.0

        # Match predicted risks with actual risks by category
        predicted_categories = {risk['category'] for risk in predicted_risks}
        tp = sum(1 for risk in actual_risks if risk['category'] in predicted_categories)
        fn = len(actual_risks) - tp

        return tp / (tp + fn) if (tp + fn) > 0 else 0.0

    @staticmethod
    def f1_score(precision: float, recall: float) -> float:
        """
        Calculate F1 score: 2 * (precision * recall) / (precision + recall)

        Args:
            precision: Precision score
            recall: Recall score

        Returns:
            F1 score (0-1)
        """
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)

    @staticmethod
    def hallucination_rate(
        generated_content: str,
        retrieved_context: List[str]
    ) -> float:
        """
        Estimate hallucination rate by checking if claims are supported by context

        Args:
            generated_content: Content generated by LLM
            retrieved_context: List of retrieved document chunks

        Returns:
            Hallucination rate (0-1)
        """
        if not generated_content:
            return 0.0

        # Simple heuristic: check if generated content contains numbers/amounts
        # that are not present in retrieved context
        import re

        # Extract numbers/amounts from generated content
        generated_numbers = set(re.findall(r'\d+[\d,.]*\d*', generated_content))
        context_numbers = set()
        for ctx in retrieved_context:
            context_numbers.update(re.findall(r'\d+[\d,.]*\d*', ctx))

        # Numbers in generated content but not in context = potential hallucinations
        hallucinations = generated_numbers - context_numbers

        if not generated_numbers:
            return 0.0

        return len(hallucinations) / len(generated_numbers)

    @staticmethod
    def citation_accuracy(
        generated_citations: List[str],
        actual_citations: List[str]
    ) -> float:
        """
        Calculate citation accuracy

        Args:
            generated_citations: Citations generated by the model
            actual_citations: Actual citations from source documents

        Returns:
            Citation accuracy (0-1)
        """
        if not generated_citations:
            return 0.0

        correct = sum(1 for gc in generated_citations if gc in actual_citations)
        return correct / len(generated_citations)

    @staticmethod
    def compute_all_metrics(
        prediction: Dict[str, Any],
        ground_truth: Dict[str, Any]
    ) -> Dict[str, MetricValue]:
        """
        Compute all evaluation metrics for a single prediction

        Args:
            prediction: Model prediction output
            ground_truth: Ground truth annotation

        Returns:
            Dictionary of metric values
        """
        metrics = {}

        # Extract findings
        predicted_risks = prediction.get('findings', [])
        actual_risks = ground_truth.get('findings', [])

        # Compute precision, recall, F1
        prec = EvaluationMetrics.precision(predicted_risks, actual_risks)
        rec = EvaluationMetrics.recall(predicted_risks, actual_risks)
        f1 = EvaluationMetrics.f1_score(prec, rec)

        metrics['precision'] = MetricValue('Precision', prec, MetricType.PRECISION)
        metrics['recall'] = MetricValue('Recall', rec, MetricType.RECALL)
        metrics['f1_score'] = MetricValue('F1 Score', f1, MetricType.F1_SCORE)

        # Compute hallucination rate
        generated_content = prediction.get('final_answer', '')
        retrieved_context = prediction.get('retrieved_context', [])
        hall_rate = EvaluationMetrics.hallucination_rate(generated_content, retrieved_context)

        metrics['hallucination_rate'] = MetricValue(
            'Hallucination Rate',
            hall_rate,
            MetricType.HALLUCINATION_RATE
        )

        # Compute citation accuracy
        generated_citations = [
            f.get('citation') for f in predicted_risks if f.get('citation')
        ]
        actual_citations = [
            f.get('citation') for f in actual_risks if f.get('citation')
        ]
        cit_acc = EvaluationMetrics.citation_accuracy(generated_citations, actual_citations)

        metrics['citation_accuracy'] = MetricValue(
            'Citation Accuracy',
            cit_acc,
            MetricType.CITATION_ACCURACY
        )

        # Extract performance metrics
        metrics['response_time'] = MetricValue(
            'Response Time',
            prediction.get('duration', 0),
            MetricType.RESPONSE_TIME,
            unit='seconds'
        )

        metrics['token_usage'] = MetricValue(
            'Token Usage',
            prediction.get('token_usage', 0),
            MetricType.TOKEN_USAGE,
            unit='tokens'
        )

        metrics['cost'] = MetricValue(
            'Cost',
            prediction.get('cost', 0),
            MetricType.COST,
            unit='USD'
        )

        return metrics


@dataclass
class RiskPoint:
    """A single risk point in a contract"""
    id: str
    category: str
    severity: str  # 'low' | 'medium' | 'high'
    description: str
    clause_text: str
    location: Optional[str] = None
    suggestion: Optional[str] = None
    citation: Optional[str] = None


@dataclass
class GroundTruthAnnotation:
    """Ground truth annotation for a contract"""
    contract_id: str
    contract_text: str
    contract_type: str
    risk_points: List[RiskPoint]
    overall_risk: str
    compliance_status: str
    metadata: Dict[str, Any]
